{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision opencv-python matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGSOUJsLmfag",
        "outputId": "54a23be0-8208-4817-b74e-fcba6438d116"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2"
      ],
      "metadata": {
        "id": "VCnvv2mxktUs"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "GSct4PpM9UlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3cf674b-7f82-4a33-ce9d-6e23b35d13dd"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример использования\n",
        "images_dir = \"/content/drive/MyDrive/3_SEMESTR/CV/CVproject/ICDAR2013/Challenge2_Training_Task12_Images\"  # Папка с изображениями\n",
        "annotations_dir = \"/content/drive/MyDrive/3_SEMESTR/CV/CVproject/ICDAR2013/Challenge2_Training_Task1_GT\"  # Папка с аннотациями\n"
      ],
      "metadata": {
        "id": "tXEesHShZk8S"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_annotation_file(file_path):\n",
        "    \"\"\"\n",
        "    Читает файл с аннотациями и возвращает список bounding boxes и меток.\n",
        "    \"\"\"\n",
        "    annotations = []\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            x1, y1, x2, y2 = map(int, parts[:4])\n",
        "            label = \" \".join(parts[4:]).strip('\"')  # Обрабатываем метку, которая может содержать пробелы\n",
        "            annotations.append((x1, y1, x2, y2, label))\n",
        "    return annotations\n"
      ],
      "metadata": {
        "id": "caEB7PHqbpyj"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "\n",
        "# Убедимся, что изображения и аннотации соответствуют друг другу\n",
        "image_files = [f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))]\n",
        "annotation_files = [f for f in os.listdir(annotations_dir) if f.endswith('.txt')]\n",
        "\n",
        "# Проверяем, что для каждого изображения есть аннотация\n",
        "valid_image_files = []\n",
        "for img_file in image_files:\n",
        "    annotation_file = f\"gt_{os.path.splitext(img_file)[0]}.txt\"\n",
        "    if annotation_file in annotation_files:\n",
        "        valid_image_files.append(img_file)\n",
        "    else:\n",
        "        print(f\"Аннотация для изображения {img_file} не найдена. Пропускаем.\")\n",
        "\n",
        "# Сортируем изображения для воспроизводимости\n",
        "valid_image_files.sort()\n",
        "\n",
        "# Разделяем изображения на обучающую и валидационную выборки\n",
        "random.seed(42)\n",
        "random.shuffle(valid_image_files)\n",
        "split_idx = int(len(valid_image_files) * 0.9)\n",
        "train_images = valid_image_files[:split_idx]\n",
        "val_images = valid_image_files[split_idx:]\n",
        "\n",
        "# Создаем списки аннотаций для обучающей и валидационной выборок\n",
        "train_annotations = []\n",
        "val_annotations = []\n",
        "\n",
        "for img_file in train_images:\n",
        "    annotation_file = f\"gt_{os.path.splitext(img_file)[0]}.txt\"\n",
        "    annotation_path = os.path.join(annotations_dir, annotation_file)\n",
        "    bboxes = parse_annotation_file(annotation_path)\n",
        "    for bbox in bboxes:\n",
        "        train_annotations.append({\n",
        "            \"image_name\": img_file,\n",
        "            \"bbox\": bbox,\n",
        "        })\n",
        "\n",
        "for img_file in val_images:\n",
        "    annotation_file = f\"gt_{os.path.splitext(img_file)[0]}.txt\"\n",
        "    annotation_path = os.path.join(annotations_dir, annotation_file)\n",
        "    bboxes = parse_annotation_file(annotation_path)\n",
        "    for bbox in bboxes:\n",
        "        val_annotations.append({\n",
        "            \"image_name\": img_file,\n",
        "            \"bbox\": bbox,\n",
        "        })\n",
        "\n",
        "# Проверяем количество изображений и аннотаций\n",
        "print(f\"Количество изображений в обучающей выборке: {len(train_images)}\")\n",
        "print(f\"Количество аннотаций в обучающей выборке: {len(train_annotations)}\")\n",
        "print(f\"Количество изображений в валидационной выборке: {len(val_images)}\")\n",
        "print(f\"Количество аннотаций в валидационной выборке: {len(val_annotations)}\")\n",
        "\n",
        "# Выводим первые 5 элементов обучающей выборки\n",
        "print(\"\\nПервые 5 элементов обучающей выборки:\")\n",
        "for i, ann in enumerate(train_annotations[:5]):\n",
        "    print(f\"Элемент {i + 1}:\")\n",
        "    print(f\"  Изображение: {ann['image_name']}\")\n",
        "    print(f\"  Bounding box: {ann['bbox']}\")\n",
        "\n",
        "# Выводим первые 5 элементов валидационной выборки\n",
        "print(\"\\nПервые 5 элементов валидационной выборки:\")\n",
        "for i, ann in enumerate(val_annotations[:5]):\n",
        "    print(f\"Элемент {i + 1}:\")\n",
        "    print(f\"  Изображение: {ann['image_name']}\")\n",
        "    print(f\"  Bounding box: {ann['bbox']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8u50OHdNbOli",
        "outputId": "ba1030e0-fe85-47bf-c699-509772cd7a16"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество изображений в обучающей выборке: 206\n",
            "Количество аннотаций в обучающей выборке: 744\n",
            "Количество изображений в валидационной выборке: 23\n",
            "Количество аннотаций в валидационной выборке: 105\n",
            "\n",
            "Первые 5 элементов обучающей выборки:\n",
            "Элемент 1:\n",
            "  Изображение: 144.jpg\n",
            "  Bounding box: (1883, 439, 2358, 648, '2.17')\n",
            "Элемент 2:\n",
            "  Изображение: 144.jpg\n",
            "  Bounding box: (1517, 698, 1814, 767, 'Knowledge')\n",
            "Элемент 3:\n",
            "  Изображение: 144.jpg\n",
            "  Bounding box: (1838, 716, 2185, 777, 'Management')\n",
            "Элемент 4:\n",
            "  Изображение: 144.jpg\n",
            "  Bounding box: (2206, 733, 2324, 793, '(KM)')\n",
            "Элемент 5:\n",
            "  Изображение: 144.jpg\n",
            "  Bounding box: (1667, 1362, 1776, 1434, 'Dr.')\n",
            "\n",
            "Первые 5 элементов валидационной выборки:\n",
            "Элемент 1:\n",
            "  Изображение: 254.jpg\n",
            "  Bounding box: (561, 105, 604, 133, '2N')\n",
            "Элемент 2:\n",
            "  Изображение: 254.jpg\n",
            "  Bounding box: (28, 118, 277, 178, '4B.522')\n",
            "Элемент 3:\n",
            "  Изображение: 254.jpg\n",
            "  Bounding box: (22, 308, 177, 348, 'Marisa')\n",
            "Элемент 4:\n",
            "  Изображение: 254.jpg\n",
            "  Bounding box: (199, 308, 399, 348, 'Bostock')\n",
            "Элемент 5:\n",
            "  Изображение: 254.jpg\n",
            "  Bounding box: (28, 375, 175, 424, 'Jenny')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для нормализации изображения\n",
        "def normalize_image(image):\n",
        "    \"\"\"\n",
        "    Нормализует изображение к диапазону [0, 1].\n",
        "    \"\"\"\n",
        "    return image / 255.0\n",
        "\n",
        "# Функция для обрезки bounding boxes\n",
        "def clamp_boxes(boxes, image_width, image_height):\n",
        "    \"\"\"\n",
        "    Обрезает bounding boxes, чтобы они не выходили за пределы изображения.\n",
        "    \"\"\"\n",
        "    boxes[:, 0] = torch.clamp(boxes[:, 0], min=0, max=image_width)  # x1\n",
        "    boxes[:, 1] = torch.clamp(boxes[:, 1], min=0, max=image_height)  # y1\n",
        "    boxes[:, 2] = torch.clamp(boxes[:, 2], min=0, max=image_width)  # x2\n",
        "    boxes[:, 3] = torch.clamp(boxes[:, 3], min=0, max=image_height)  # y2\n",
        "    return boxes\n",
        "\n",
        "def filter_invalid_boxes(boxes):\n",
        "    \"\"\"\n",
        "    Удаляет bounding boxes с нулевой шириной или высотой.\n",
        "    \"\"\"\n",
        "    widths = boxes[:, 2] - boxes[:, 0]\n",
        "    heights = boxes[:, 3] - boxes[:, 1]\n",
        "    valid_indices = (widths > 0) & (heights > 0)\n",
        "    return boxes[valid_indices]"
      ],
      "metadata": {
        "id": "DhDLb7mV5WwO"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс датасета\n",
        "class ICDARDataset(Dataset):\n",
        "    def __init__(self, root, annotations, resize_to=(800, 800)):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root (str): Путь к папке с изображениями.\n",
        "            annotations (list): Список аннотаций.\n",
        "            resize_to (tuple): Размер, к которому будут изменены изображения и bounding boxes.\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.annotations = annotations\n",
        "        self.imgs = list(set([ann[\"image_name\"] for ann in annotations]))  # Уникальные имена изображений\n",
        "        self.resize_to = resize_to\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Загрузка изображения\n",
        "        img_name = self.imgs[idx]\n",
        "        img_path = os.path.join(self.root, img_name)\n",
        "        img = cv2.imread(img_path)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Преобразуем в RGB\n",
        "        img = normalize_image(img)  # Нормализация\n",
        "        img_height, img_width = img.shape[:2]\n",
        "\n",
        "        # Получаем аннотации для текущего изображения\n",
        "        anns = [ann for ann in self.annotations if ann[\"image_name\"] == img_name]\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        for ann in anns:\n",
        "            x1, y1, x2, y2, label = ann[\"bbox\"]\n",
        "            boxes.append([x1, y1, x2, y2])\n",
        "            labels.append(1)  # Все объекты относятся к классу \"text\"\n",
        "\n",
        "        # Преобразуем bounding boxes в тензор\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "\n",
        "        # Обрезаем bounding boxes, чтобы они не выходили за пределы изображения\n",
        "        boxes = clamp_boxes(boxes, img_width, img_height)\n",
        "\n",
        "        # Удаляем некорректные bounding boxes\n",
        "        boxes = filter_invalid_boxes(boxes)\n",
        "\n",
        "        # Если bounding boxes отсутствуют, пропускаем это изображение\n",
        "        if len(boxes) == 0:\n",
        "            print(f\"Нет валидных bounding boxes в изображении {img_name}. Пропускаем.\")\n",
        "            return None, None\n",
        "\n",
        "        # Изменяем размер изображения и bounding boxes\n",
        "        img, boxes = self.resize_image_and_boxes(img, boxes)\n",
        "\n",
        "        # Преобразуем изображение в тензор (C, H, W)\n",
        "        img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
        "\n",
        "        # Создаем target (целевые значения для модели)\n",
        "        target = {\n",
        "            \"boxes\": boxes,  # Bounding boxes в формате [x_min, y_min, x_max, y_max]\n",
        "            \"labels\": torch.as_tensor(labels[:len(boxes)], dtype=torch.int64),  # Метки классов\n",
        "            \"image_id\": torch.tensor([idx]),  # Уникальный идентификатор изображения\n",
        "            \"area\": (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]),  # Площадь bounding boxes\n",
        "            \"iscrowd\": torch.zeros((len(boxes),), dtype=torch.int64),  # Флаг \"iscrowd\" (0 для отдельных объектов)\n",
        "        }\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def resize_image_and_boxes(self, image, boxes):\n",
        "        \"\"\"\n",
        "        Изменяет размер изображения и масштабирует bounding boxes.\n",
        "        \"\"\"\n",
        "        orig_height, orig_width = image.shape[:2]\n",
        "        new_height, new_width = self.resize_to\n",
        "\n",
        "        # Изменяем размер изображения\n",
        "        image = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "        # Масштабируем bounding boxes\n",
        "        scale_x = new_width / orig_width\n",
        "        scale_y = new_height / orig_height\n",
        "        boxes[:, [0, 2]] *= scale_x\n",
        "        boxes[:, [1, 3]] *= scale_y\n",
        "\n",
        "        return image, boxes"
      ],
      "metadata": {
        "id": "Wbx3AA75ktSZ"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_consistency(dataset):\n",
        "    for i in range(len(dataset)):\n",
        "        img, target = dataset[i]\n",
        "        boxes = target['boxes']\n",
        "        labels = target['labels']\n",
        "\n",
        "        # Проверка координат bounding boxes\n",
        "        if (boxes[:, 0] < 0).any() or (boxes[:, 1] < 0).any() or \\\n",
        "           (boxes[:, 2] > img.shape[2]).any() or (boxes[:, 3] > img.shape[1]).any():\n",
        "            print(f\"Некорректные bounding boxes в изображении {dataset.imgs[i]}: {boxes}\")\n",
        "\n",
        "        # Проверка меток\n",
        "        if (labels < 0).any():\n",
        "            print(f\"Некорректные метки в изображении {dataset.imgs[i]}: {labels}\")"
      ],
      "metadata": {
        "id": "6EOJjmLwNZdV"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ICDARDataset(root=images_dir, annotations=train_annotations, resize_to=(800, 800))\n",
        "val_dataset = ICDARDataset(root=images_dir, annotations=val_annotations, resize_to=(800, 800))\n",
        "\n",
        "# Проверяем корректность данных\n",
        "check_data_consistency(train_dataset)\n",
        "check_data_consistency(val_dataset)\n",
        "\n",
        "def print_random_samples(dataset, name, num_samples=3):\n",
        "    \"\"\"\n",
        "    Выводит информацию о случайных элементах датасета.\n",
        "    \"\"\"\n",
        "    print(f\"Случайные элементы датасета {name}:\")\n",
        "    indices = random.sample(range(len(dataset)), num_samples)\n",
        "    for i, idx in enumerate(indices):\n",
        "        img, target = dataset[idx]\n",
        "        print(f\"Элемент {i + 1}:\")\n",
        "        print(f\"  Изображение: {dataset.imgs[idx]}\")\n",
        "        print(f\"  Форма изображения: {img.shape}\")\n",
        "        print(f\"  Количество bounding boxes: {len(target['boxes'])}\")\n",
        "        if len(target['boxes']) > 0:\n",
        "            print(f\"  Пример bounding box: {target['boxes'][0]}\")\n",
        "        else:\n",
        "            print(\"  Пример bounding box: Нет bounding boxes\")\n",
        "        print(f\"  Метки: {target['labels']}\")\n",
        "        print(f\"  Площадь: {target['area']}\")\n",
        "        print(f\"  iscrowd: {target['iscrowd']}\")\n",
        "        print(f\"  ID изображения: {target['image_id']}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Вывод информации о 3 случайных элементах каждого датасета\n",
        "print_random_samples(train_dataset, \"Train\")\n",
        "print_random_samples(val_dataset, \"Validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHYty4NDb47L",
        "outputId": "adafe74b-7204-4d85-a6d4-da14faa38039"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Случайные элементы датасета Train:\n",
            "Элемент 1:\n",
            "  Изображение: 132.jpg\n",
            "  Форма изображения: torch.Size([3, 800, 800])\n",
            "  Количество bounding boxes: 10\n",
            "  Пример bounding box: tensor([ 59.6708, 478.3951, 141.9753, 545.6790])\n",
            "  Метки: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "  Площадь: tensor([ 5537.7754,  9263.8906,  5161.8145, 16881.5723, 11630.4717,  6757.9302,\n",
            "         6014.7051, 23430.9395,  6469.4800,  9620.9893])\n",
            "  iscrowd: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "  ID изображения: tensor([54])\n",
            "\n",
            "\n",
            "Элемент 2:\n",
            "  Изображение: 234.jpg\n",
            "  Форма изображения: torch.Size([3, 800, 800])\n",
            "  Количество bounding boxes: 7\n",
            "  Пример bounding box: tensor([116.8750, 264.1667, 180.0000, 309.1667])\n",
            "  Метки: tensor([1, 1, 1, 1, 1, 1, 1])\n",
            "  Площадь: tensor([ 2840.6250,  7612.4966,  6750.0000,  4542.1875,  7233.3320,  1884.3750,\n",
            "        13952.0879])\n",
            "  iscrowd: tensor([0, 0, 0, 0, 0, 0, 0])\n",
            "  ID изображения: tensor([138])\n",
            "\n",
            "\n",
            "Элемент 3:\n",
            "  Изображение: 171.jpg\n",
            "  Форма изображения: torch.Size([3, 800, 800])\n",
            "  Количество bounding boxes: 1\n",
            "  Пример bounding box: tensor([ 60.0000, 388.3333, 728.7500, 530.0000])\n",
            "  Метки: tensor([1])\n",
            "  Площадь: tensor([94739.5938])\n",
            "  iscrowd: tensor([0])\n",
            "  ID изображения: tensor([33])\n",
            "\n",
            "\n",
            "Случайные элементы датасета Validation:\n",
            "Элемент 1:\n",
            "  Изображение: 208.jpg\n",
            "  Форма изображения: torch.Size([3, 800, 800])\n",
            "  Количество bounding boxes: 3\n",
            "  Пример bounding box: tensor([342.5000, 371.6667, 416.2500, 428.3333])\n",
            "  Метки: tensor([1, 1, 1])\n",
            "  Площадь: tensor([ 4179.1660, 10864.5850, 14349.9941])\n",
            "  iscrowd: tensor([0, 0, 0])\n",
            "  ID изображения: tensor([18])\n",
            "\n",
            "\n",
            "Элемент 2:\n",
            "  Изображение: 162.jpg\n",
            "  Форма изображения: torch.Size([3, 800, 800])\n",
            "  Количество bounding boxes: 4\n",
            "  Пример bounding box: tensor([114.5000, 300.6667, 254.5000, 338.6667])\n",
            "  Метки: tensor([1, 1, 1, 1])\n",
            "  Площадь: tensor([5320.0000, 5637.3301, 5890.0000, 4732.6680])\n",
            "  iscrowd: tensor([0, 0, 0, 0])\n",
            "  ID изображения: tensor([15])\n",
            "\n",
            "\n",
            "Элемент 3:\n",
            "  Изображение: 288.jpg\n",
            "  Форма изображения: torch.Size([3, 800, 800])\n",
            "  Количество bounding boxes: 15\n",
            "  Пример bounding box: tensor([122.4280, 476.2346, 185.5967, 523.7654])\n",
            "  Метки: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "  Площадь: tensor([ 3002.4656, 11053.4414,  6349.4512,  9890.2588,  7441.1304,  9495.5049,\n",
            "        11072.4346, 11599.5391,   857.9751,   718.3873,   384.0877,  1549.3048,\n",
            "          683.3939,   400.5998,   268.9489])\n",
            "  iscrowd: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "  ID изображения: tensor([7])\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Объединяет данные в батчи, пропуская изображения без валидных bounding boxes.\n",
        "    \"\"\"\n",
        "    batch = [item for item in batch if item[0] is not None]  # Пропускаем None\n",
        "    if len(batch) == 0:\n",
        "        return None, None  # Если все изображения в батче пропущены\n",
        "    images = [item[0] for item in batch]\n",
        "    targets = [item[1] for item in batch]\n",
        "    return images, targets"
      ],
      "metadata": {
        "id": "Eihbxjz30Q4S"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "JiNjZtLngfdJ"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Рассчитывает Intersection over Union (IoU) для двух bounding boxes.\n",
        "    \"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    iou = inter_area / (box1_area + box2_area - inter_area + 1e-9)\n",
        "    return iou\n",
        "\n",
        "def calculate_metrics(pred_boxes, true_boxes, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Рассчитывает Precision, Recall и F1-score для одного изображения.\n",
        "    \"\"\"\n",
        "    if len(pred_boxes) == 0:\n",
        "        return 0.0, 0.0, 0.0  # Если нет предсказаний, метрики равны 0\n",
        "\n",
        "    if len(true_boxes) == 0:\n",
        "        return 0.0, 0.0, 0.0  # Если нет истинных объектов, метрики равны 0\n",
        "\n",
        "    # Матрица IoU между предсказанными и истинными bounding boxes\n",
        "    iou_matrix = torch.zeros((len(pred_boxes), len(true_boxes)))\n",
        "    for i, pred_box in enumerate(pred_boxes):\n",
        "        for j, true_box in enumerate(true_boxes):\n",
        "            iou_matrix[i, j] = calculate_iou(pred_box, true_box)\n",
        "\n",
        "    # Жадное сопоставление: каждый истинный объект сопоставляется с одним предсказанным bounding box\n",
        "    tp = 0\n",
        "    matched_true_indices = set()  # Индексы истинных объектов, которые уже сопоставлены\n",
        "    matched_pred_indices = set()  # Индексы предсказанных объектов, которые уже сопоставлены\n",
        "\n",
        "    # Сначала сопоставляем предсказания с истинными объектами\n",
        "    for j in range(len(true_boxes)):\n",
        "        max_iou_idx = torch.argmax(iou_matrix[:, j]).item()\n",
        "        if iou_matrix[max_iou_idx, j] >= iou_threshold and max_iou_idx not in matched_pred_indices:\n",
        "            tp += 1\n",
        "            matched_true_indices.add(j)\n",
        "            matched_pred_indices.add(max_iou_idx)\n",
        "\n",
        "    # Подсчет FP и FN\n",
        "    fp = len(pred_boxes) - len(matched_pred_indices)  # False Positives\n",
        "    fn = len(true_boxes) - len(matched_true_indices)  # False Negatives\n",
        "\n",
        "    # Расчет метрик\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-9)  # Добавляем 1e-9 для избежания деления на 0\n",
        "\n",
        "    return precision, recall, f1\n",
        "\n",
        "def calculate_batch_metrics(preds, targets, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Рассчитывает средние Precision, Recall и F1-score для батча.\n",
        "    \"\"\"\n",
        "    batch_precision = []\n",
        "    batch_recall = []\n",
        "    batch_f1 = []\n",
        "\n",
        "    for pred, target in zip(preds, targets):\n",
        "        pred_boxes = pred['boxes'].cpu().numpy()\n",
        "        true_boxes = target['boxes'].cpu().numpy()\n",
        "\n",
        "        precision, recall, f1 = calculate_metrics(pred_boxes, true_boxes, iou_threshold)\n",
        "        batch_precision.append(precision)\n",
        "        batch_recall.append(recall)\n",
        "        batch_f1.append(f1)\n",
        "\n",
        "    # Средние значения метрик по батчу\n",
        "    avg_precision = sum(batch_precision) / len(batch_precision)\n",
        "    avg_recall = sum(batch_recall) / len(batch_recall)\n",
        "    avg_f1 = sum(batch_f1) / len(batch_f1)\n",
        "\n",
        "    return avg_precision, avg_recall, avg_f1"
      ],
      "metadata": {
        "id": "QikgqEeJb5YH"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)"
      ],
      "metadata": {
        "id": "Rtt7-wKrb44L"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, val_dataloader, num_epochs=10, lr=0.001):\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
        "    train_losses = []\n",
        "    metrics = {'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_train_loss = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        for images, targets in train_dataloader:\n",
        "            if images is None:  # Пропускаем пустые батчи\n",
        "                continue\n",
        "\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            # Вызов модели\n",
        "            loss_dict = model(images, targets)\n",
        "\n",
        "            # Суммируем потери из словаря\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "            epoch_train_loss += losses.item()\n",
        "\n",
        "        train_losses.append(epoch_train_loss / len(train_dataloader))\n",
        "\n",
        "        # Валидация\n",
        "        model.eval()\n",
        "        epoch_precision = 0\n",
        "        epoch_recall = 0\n",
        "        epoch_f1 = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, targets in val_dataloader:\n",
        "                if images is None:  # Пропускаем пустые батчи\n",
        "                    continue\n",
        "\n",
        "                images = list(image.to(device) for image in images)\n",
        "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "                # Вызов модели (в режиме оценки возвращает список предсказаний)\n",
        "                preds = model(images)\n",
        "\n",
        "                # Расчет метрик\n",
        "                for pred, target in zip(preds, targets):\n",
        "                    pred_boxes = pred['boxes'].cpu().numpy()\n",
        "                    true_boxes = target['boxes'].cpu().numpy()\n",
        "                    precision, recall, f1 = calculate_metrics(pred_boxes, true_boxes)\n",
        "                    epoch_precision += precision\n",
        "                    epoch_recall += recall\n",
        "                    epoch_f1 += f1\n",
        "\n",
        "        # Средние значения метрик\n",
        "        metrics['precision'].append(epoch_precision / len(val_dataloader))\n",
        "        metrics['recall'].append(epoch_recall / len(val_dataloader))\n",
        "        metrics['f1'].append(epoch_f1 / len(val_dataloader))\n",
        "\n",
        "        # Вывод результатов\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {train_losses[-1]:.4f}\")\n",
        "        print(f\"Precision: {metrics['precision'][-1]:.4f}, Recall: {metrics['recall'][-1]:.4f}, F1: {metrics['f1'][-1]:.4f}\")\n",
        "        print(f\"Time: {time.time() - start_time:.2f}s\")\n",
        "\n",
        "    # Построение графика loss\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Сохранение модели\n",
        "    torch.save(model.state_dict(), \"faster_rcnn_text_detection.pth\")\n",
        "    print(\"Модель сохранена как 'faster_rcnn_text_detection.pth'\")\n",
        "\n",
        "    return model, train_losses, metrics"
      ],
      "metadata": {
        "id": "cIjY2wXCu09t"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8Or7-fmPSEF"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_xg6ngMrPWMh"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, train_dataloader, val_dataloader, num_epochs=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "1fWXsvml1Ypm",
        "outputId": "f5900bf0-176f-4c47-827b-61728542c371"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e94646b693ad>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0l9DNVm8ktB3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQOJwCpIrlg_"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}