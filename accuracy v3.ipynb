{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T11:30:01.345775Z",
     "start_time": "2025-01-17T11:29:26.514020Z"
    }
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import easyocr\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from pathlib import Path\n",
    "from difflib import SequenceMatcher\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:09:16.178010Z",
     "start_time": "2025-01-17T11:08:56.889784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Пути\n",
    "test_images_folder = Path(r\"C:\\Users\\abram\\DataspellProjects\\FinetuningEasyOCR\\dataset\\test\")\n",
    "test_labels_folder = Path(r\"C:\\Users\\abram\\DataspellProjects\\FinetuningEasyOCR\\dataset\\ch4_test_localization_transcription_gt\")\n",
    "faster_rcnn_weights = Path(r\"C:\\Users\\abram\\DataspellProjects\\FinetuningEasyOCR\\faster_rcnn_text_detection_100epoch.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Загрузка модели Faster R-CNN\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2)  # 2 класса: фон и текст\n",
    "model.load_state_dict(torch.load(faster_rcnn_weights, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Инициализация EasyOCR\n",
    "reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())"
   ],
   "id": "5a54a21a28cea200",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abram\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\abram\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\abram/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:14<00:00, 7.01MB/s]\n",
      "C:\\Users\\abram\\AppData\\Local\\Temp\\ipykernel_34560\\4234307793.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(faster_rcnn_weights, map_location=device))\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:10:05.686192Z",
     "start_time": "2025-01-17T11:10:05.680172Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Функция для загрузки ground truth\n",
    "def load_ground_truths(labels_folder):\n",
    "    ground_truths = {}\n",
    "    bboxes = {}\n",
    "    for label_file in Path(labels_folder).iterdir():\n",
    "        if label_file.suffix == \".txt\":\n",
    "            image_name = label_file.stem.replace(\"gt_\", \"\") + \".jpg\"\n",
    "            with label_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                gt_texts = []\n",
    "                gt_bboxes = []\n",
    "                for line in f.readlines():\n",
    "                    parts = line.strip().split(\",\")\n",
    "                    if parts[-1] != \"###\":  # Игнорируем метки с ###\n",
    "                        x_min, y_min, x_max, y_max = map(float, parts[:4])\n",
    "                        text = parts[-1].lower()\n",
    "                        gt_texts.append(text)\n",
    "                        gt_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "                ground_truths[image_name] = gt_texts\n",
    "                bboxes[image_name] = gt_bboxes\n",
    "    return ground_truths, bboxes\n",
    "\n",
    "# Функция для получения предсказаний Faster R-CNN + EasyOCR\n",
    "def get_faster_rcnn_easyocr_predictions(image_folder, model, reader):\n",
    "    predictions = {}\n",
    "    bboxes = {}\n",
    "    for image_file in Path(image_folder).iterdir():\n",
    "        if image_file.suffix.lower() in [\".jpg\", \".png\", \".jpeg\"]:\n",
    "            # Загрузка изображения\n",
    "            image = cv2.imread(str(image_file))\n",
    "            orig_image = image.copy()\n",
    "            image = F.to_tensor(image).to(device)\n",
    "\n",
    "            # Faster R-CNN: детекция текстовых областей\n",
    "            with torch.no_grad():\n",
    "                outputs = model([image])[0]\n",
    "\n",
    "            predicted_words = []\n",
    "            predicted_bboxes = []\n",
    "\n",
    "            for box, score in zip(outputs['boxes'], outputs['scores']):\n",
    "                if score >= 0.5:  # Порог уверенности\n",
    "                    x_min, y_min, x_max, y_max = map(int, box.tolist())\n",
    "                    cropped_image = orig_image[y_min:y_max, x_min:x_max]\n",
    "                    ocr_results = reader.readtext(cropped_image)\n",
    "                    for _, text, _ in ocr_results:\n",
    "                        predicted_words.append(text.lower())\n",
    "                        predicted_bboxes.append([x_min, y_min, x_max, y_max])\n",
    "\n",
    "            predictions[image_file.name] = predicted_words\n",
    "            bboxes[image_file.name] = predicted_bboxes\n",
    "\n",
    "    return predictions, bboxes"
   ],
   "id": "bdf611273f79ece5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:13:34.028174Z",
     "start_time": "2025-01-17T11:13:34.023703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Должно вернуть True\n",
    "print(torch.cuda.current_device())  # Текущий активный GPU\n",
    "print(torch.cuda.get_device_name(0))  # Имя GPU"
   ],
   "id": "12220dd5d6b12f24",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Метрики CER и WER\n",
    "def calculate_cer(gt_texts, pred_texts):\n",
    "    cer_scores = []\n",
    "    for gt, pred in zip(gt_texts, pred_texts):\n",
    "        sm = SequenceMatcher(None, gt, pred)\n",
    "        edit_distance = sum(\n",
    "            (i2 - i1) if tag != 'insert' else (j2 - j1)\n",
    "            for tag, i1, i2, j1, j2 in sm.get_opcodes()\n",
    "            if tag != 'equal'\n",
    "        )\n",
    "        cer = edit_distance / max(len(gt), 1)\n",
    "        cer_scores.append(cer)\n",
    "    return np.mean(cer_scores)\n",
    "\n",
    "def calculate_wer(gt_texts, pred_texts):\n",
    "    wer_scores = []\n",
    "    for gt, pred in zip(gt_texts, pred_texts):\n",
    "        gt_words = gt.split()\n",
    "        pred_words = pred.split()\n",
    "        sm = SequenceMatcher(None, gt_words, pred_words)\n",
    "        edit_distance = sum(\n",
    "            (i2 - i1) if tag != 'insert' else (j2 - j1)\n",
    "            for tag, i1, i2, j1, j2 in sm.get_opcodes()\n",
    "            if tag != 'equal'\n",
    "        )\n",
    "        wer = edit_distance / max(len(gt_words), 1)\n",
    "        wer_scores.append(wer)\n",
    "    return np.mean(wer_scores)\n",
    "\n",
    "# Метрика IoU\n",
    "def calculate_iou(gt_bboxes, pred_bboxes):\n",
    "    def iou(box1, box2):\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "        box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "        box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "        union_area = box1_area + box2_area - inter_area\n",
    "        return inter_area / union_area if union_area > 0 else 0\n",
    "\n",
    "    iou_scores = []\n",
    "    for gt_boxes, pred_boxes in zip(gt_bboxes.values(), pred_bboxes.values()):\n",
    "        for gt, pred in zip(gt_boxes, pred_boxes):\n",
    "            iou_scores.append(iou(gt, pred))\n",
    "    return np.mean(iou_scores)"
   ],
   "id": "5ceff41f9e931162"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T11:13:45.277219Z",
     "start_time": "2025-01-17T11:13:44.878621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Основная логика\n",
    "if __name__ == \"__main__\":\n",
    "    # Загрузка ground truth для оценки\n",
    "    ground_truths, gt_bboxes = load_ground_truths(test_labels_folder)\n",
    "\n",
    "    # Получение предсказаний Faster R-CNN + EasyOCR\n",
    "    faster_rcnn_predictions, pred_bboxes = get_faster_rcnn_easyocr_predictions(test_images_folder, model, reader)\n",
    "\n",
    "    # Вычисление метрик\n",
    "    cer = calculate_cer(\n",
    "        [\" \".join(gt) for gt in ground_truths.values()],\n",
    "        [\" \".join(pred) for pred in faster_rcnn_predictions.values()]\n",
    "    )\n",
    "    wer = calculate_wer(\n",
    "        [\" \".join(gt) for gt in ground_truths.values()],\n",
    "        [\" \".join(pred) for pred in faster_rcnn_predictions.values()]\n",
    "    )\n",
    "    iou = calculate_iou(gt_bboxes, pred_bboxes)\n",
    "\n",
    "    print(f\"CER: {cer:.4f}\")\n",
    "    print(f\"WER: {wer:.4f}\")\n",
    "    print(f\"IoU: {iou:.4f}\")\n"
   ],
   "id": "588bb3bcefef4f5a",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m ground_truths, gt_bboxes \u001B[38;5;241m=\u001B[39m load_ground_truths(test_labels_folder)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Получение предсказаний Faster R-CNN + EasyOCR\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m faster_rcnn_predictions, pred_bboxes \u001B[38;5;241m=\u001B[39m \u001B[43mget_faster_rcnn_easyocr_predictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_images_folder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# Вычисление метрик\u001B[39;00m\n\u001B[0;32m     10\u001B[0m cer \u001B[38;5;241m=\u001B[39m calculate_cer(\n\u001B[0;32m     11\u001B[0m     [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(gt) \u001B[38;5;28;01mfor\u001B[39;00m gt \u001B[38;5;129;01min\u001B[39;00m ground_truths\u001B[38;5;241m.\u001B[39mvalues()],\n\u001B[0;32m     12\u001B[0m     [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(pred) \u001B[38;5;28;01mfor\u001B[39;00m pred \u001B[38;5;129;01min\u001B[39;00m faster_rcnn_predictions\u001B[38;5;241m.\u001B[39mvalues()]\n\u001B[0;32m     13\u001B[0m )\n",
      "Cell \u001B[1;32mIn[5], line 35\u001B[0m, in \u001B[0;36mget_faster_rcnn_easyocr_predictions\u001B[1;34m(image_folder, model, reader)\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# Faster R-CNN: детекция текстовых областей\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 35\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     37\u001B[0m predicted_words \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     38\u001B[0m predicted_bboxes \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001B[0m, in \u001B[0;36mGeneralizedRCNN.forward\u001B[1;34m(self, images, targets)\u001B[0m\n\u001B[0;32m    103\u001B[0m     features \u001B[38;5;241m=\u001B[39m OrderedDict([(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m0\u001B[39m\u001B[38;5;124m\"\u001B[39m, features)])\n\u001B[0;32m    104\u001B[0m proposals, proposal_losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrpn(images, features, targets)\n\u001B[1;32m--> 105\u001B[0m detections, detector_losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mroi_heads\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproposals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimages\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimage_sizes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    106\u001B[0m detections \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform\u001B[38;5;241m.\u001B[39mpostprocess(detections, images\u001B[38;5;241m.\u001B[39mimage_sizes, original_image_sizes)  \u001B[38;5;66;03m# type: ignore[operator]\u001B[39;00m\n\u001B[0;32m    108\u001B[0m losses \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:761\u001B[0m, in \u001B[0;36mRoIHeads.forward\u001B[1;34m(self, features, proposals, image_shapes, targets)\u001B[0m\n\u001B[0;32m    758\u001B[0m     regression_targets \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    759\u001B[0m     matched_idxs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 761\u001B[0m box_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbox_roi_pool\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproposals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_shapes\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    762\u001B[0m box_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbox_head(box_features)\n\u001B[0;32m    763\u001B[0m class_logits, box_regression \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbox_predictor(box_features)\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torchvision\\ops\\poolers.py:314\u001B[0m, in \u001B[0;36mMultiScaleRoIAlign.forward\u001B[1;34m(self, x, boxes, image_shapes)\u001B[0m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscales \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap_levels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    310\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscales, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmap_levels \u001B[38;5;241m=\u001B[39m _setup_scales(\n\u001B[0;32m    311\u001B[0m         x_filtered, image_shapes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcanonical_scale, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcanonical_level\n\u001B[0;32m    312\u001B[0m     )\n\u001B[1;32m--> 314\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_multiscale_roi_align\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_filtered\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mboxes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msampling_ratio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscales\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_levels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    321\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\DataspellProjects\\FinetuningEasyOCR\\.venv\\Lib\\site-packages\\torchvision\\ops\\poolers.py:201\u001B[0m, in \u001B[0;36m_multiscale_roi_align\u001B[1;34m(x_filtered, boxes, output_size, sampling_ratio, scales, mapper)\u001B[0m\n\u001B[0;32m    199\u001B[0m tracing_results \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m level, (per_level_feature, scale) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mzip\u001B[39m(x_filtered, scales)):\n\u001B[1;32m--> 201\u001B[0m     idx_in_level \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mwhere(\u001B[43mlevels\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    202\u001B[0m     rois_per_level \u001B[38;5;241m=\u001B[39m rois[idx_in_level]\n\u001B[0;32m    204\u001B[0m     result_idx_in_level \u001B[38;5;241m=\u001B[39m roi_align(\n\u001B[0;32m    205\u001B[0m         per_level_feature,\n\u001B[0;32m    206\u001B[0m         rois_per_level,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    209\u001B[0m         sampling_ratio\u001B[38;5;241m=\u001B[39msampling_ratio,\n\u001B[0;32m    210\u001B[0m     )\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: unknown error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b9d6d5afd94306d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
